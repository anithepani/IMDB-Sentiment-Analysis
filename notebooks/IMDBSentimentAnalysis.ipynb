<a href="https://colab.research.google.com/github/M-SAAD-BIN-MAZHAR/SentimentAnalysis/blob/master/IMDBSentimentAnalysis.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

from google.colab import files
files.upload()  # Choose kaggle.json when prompted


!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json


!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews


!unzip imdb-dataset-of-50k-movie-reviews.zip



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/IMDB Dataset.csv')

df.head(10)

df.info()

df.isnull().sum()

df.duplicated().sum()

df.shape

df.drop_duplicates(inplace=True)

df.shape





df['sentiment'].value_counts() #Balanced Data Set

df.describe()

# **IMPORTING IMPORTANT LIBRARIES**

!pip install gensim


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import string
nltk.download('stopwords')
import re
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
nltk.download('punkt')
nltk.download('wordnet')
from bs4 import BeautifulSoup
from sklearn.naive_bayes import MultinomialNB,GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from wordcloud import WordCloud
from sklearn.linear_model import SGDClassifier
import gensim
from gensim.models import Word2Vec,KeyedVectors
stopwords=set(stopwords.words('english'))
nltk.download('punkt_tab')

# **CLEANING OF TEXT**

df['review'][8]

def strip_html(text):
  soap=BeautifulSoup(text,'html.parser')
  return soap.get_text()
def remove_between_square_brackets(text):
  return re.sub(r'\[[^]]*\]','',text)
def remove_special_characters(text,remove_digits=True):
  pattern=r'[^a-zA-z0-9\s]'
  text=re.sub(pattern,'',text)
  return text
def apply_stemmer(text): #if reult is bad check lematizer
  ps=PorterStemmer()
  return " ".join([ps.stem(word) for word in text.split()])
  return text
def remove_stopwords(text):
    return ' '.join(word for word in text.split() if word not in stopwords)


def denoise_text(text):
  text=text.lower()
  text=strip_html(text)
  text=remove_between_square_brackets(text)
  text=remove_special_characters(text)
  text=apply_stemmer(text)
  text=remove_stopwords(text)
  return text

df['review']=df['review'].apply(denoise_text)

df['review'][0]

# **SPLITTING OF DATASET**

x_train,x_test,y_train,y_test=train_test_split(df['review'],df['sentiment'],test_size=0.2,random_state=42)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# **BAG OF WORDS**

cv=CountVectorizer()
x_train_cv=cv.fit_transform(x_train)
x_test_cv=cv.transform(x_test)
print(x_train_cv.shape)
print(x_test_cv.shape)

# **TFIDF VECTORIZER**

tf=TfidfVectorizer()
x_train_tf=tf.fit_transform(x_train)
x_test_tf=tf.transform(x_test)
print(x_train_tf.shape)
print(x_test_tf.shape)

# **Word2Vec**

sentences = df['review'].apply(nltk.word_tokenize).tolist()

#For using word2vec it is import to use it before word_tokenizer
model=gensim.models.Word2Vec(window=10,min_count=2,workers=4)
model.build_vocab(sentences,progress_per=1000)
model.train(sentences,total_examples=model.corpus_count,epochs=model.epochs)

# **Labeling Sentiments**

lb=LabelEncoder()
y_train=lb.fit_transform(y_train)
y_test=lb.transform(y_test)

y_train.shape

# **Applying Algorithms**

# **Logisitic Regression Using Bag of words**

lr=LogisticRegression(max_iter=600)
lr.fit(x_train_cv,y_train)
y_pred=lr.predict(x_test_cv)
print(accuracy_score(y_test,y_pred))

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

# **Logisitic Regression Using Tfidf**

lr=LogisticRegression(max_iter=600)
lr.fit(x_train_tf,y_train)
y_pred=lr.predict(x_test_tf)
print(accuracy_score(y_test,y_pred))

confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

# **Logistic Regression Using Word2Vec**



def get_avg_vector(tokens, model):
    vectors = [model.wv[w] for w in tokens if w in model.wv]
    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)

# Convert each tokenized review into one vector
X = np.vstack([get_avg_vector(tokens, model) for tokens in sentences])


y = (df['sentiment'] == 'positive').astype(int).values


X_train_wv, X_test_wv, y_train_wv, y_test_wv = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Logistic Regression
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train_wv, y_train_wv)

print("Training Accuracy:", clf.score(X_train_wv, y_train_wv))
print("Test Accuracy:", clf.score(X_test_wv, y_test_wv))

# **MultinomialNB Classifier**

mb=MultinomialNB()
mb.fit(x_train_cv,y_train)
y_pred=mb.predict(x_test_cv)
print(accuracy_score(y_test,y_pred))




confusion_matrix(y_test,y_pred)

print(classification_report(y_test,y_pred))

# **MultinomialNB Classifier using Tfidf**

mb=MultinomialNB()
mb.fit(x_train_tf,y_train)
y_pred=mb.predict(x_test_tf)
print(accuracy_score(y_test,y_pred))

gb=GaussianNB()
gb.fit(X_train_wv, y_train_wv)

print("Training Accuracy:", gb.score(X_train_wv, y_train_wv))
print("Test Accuracy:", gb.score(X_test_wv, y_test_wv))



rd=RandomForestClassifier()
rd.fit(x_train_cv,y_train)
y_pred=rd.predict(x_test_cv)
print(accuracy_score(y_test,y_pred))

print(classification_report(y_test,y_pred))

confusion_matrix(y_test,y_pred)

rd=RandomForestClassifier()
rd.fit(x_train_tf,y_train)
y_pred=rd.predict(x_test_tf)
print(accuracy_score(y_test,y_pred))

svc=SVC()
svc.fit(x_train_cv,y_train)
y_pred=svc.predict(x_test_cv)
print(accuracy_score(y_test,y_pred)) #You can use but taking too much time



print(classification_report(y_test,y_pred))

print(confusion_matrix(y_test,y_pred))

sdc=SGDClassifier()
sdc.fit(x_train_cv,y_train)
y_pred=sdc.predict(x_test_cv)
print(accuracy_score(y_test,y_pred))



# **500 MOST FREQUENT  POSITIVE WORDS**

positive_reviews = df[df['sentiment'] == 'positive']
positive_text = " ".join(review for review in positive_reviews.review)
WC = WordCloud(width=1000, height=500, max_words=500, min_font_size=5).generate(positive_text)

plt.figure(figsize=(10,10))
plt.imshow(WC, interpolation='bilinear')
plt.axis("off")
plt.show()


# **500 MOST FREQUENT NEGATIVE WORDS**

positive_reviews = df[df['sentiment'] == 'negative']
positive_text = " ".join(review for review in positive_reviews.review)
WC = WordCloud(width=1000, height=500, max_words=500, min_font_size=5).generate(positive_text)

plt.figure(figsize=(10,10))
plt.imshow(WC, interpolation='bilinear')
plt.axis("off")
plt.show()

import pickle

pickle.dump(lr,open('lr.pkl','wb'))
pickle.dump(tf,open('tf.pkl','wb'))

def predict(text):

  cleaned_text=denoise_text(text)
  tf_text=tf.transform([cleaned_text])
  return lr.predict(tf_text)[0]



predict('basic famili littl boy jake think zombi hi closet hi parent fight timethi movi slower soap opera suddenli jake decid becom rambo kill zombieok first go make film must decid thriller drama drama movi watchabl parent divorc argu like real life jake hi closet total ruin film expect see boogeyman similar movi instead watch drama meaningless thriller spots3 10 well play parent descent dialog shot jake ignor')

predict('probabl alltim favorit movi stori selfless sacrific dedic nobl caus preachi bore never get old despit seen 15 time last 25 year paul luka perform bring tear eye bett davi one veri truli sympathet role delight kid grandma say like dressedup midget children onli make fun watch mother slow awaken happen world roof believ startl dozen thumb theyd thi movi')

predict('Movie pixel was a litle bit dull')

#0==>negative
#1==>Postive
df.sample(5)

